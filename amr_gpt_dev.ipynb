{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/Notebooks/blob/master/amr_gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT is interesting. It sequentially generates text based on prompts. And it does so slightly differently every time.<br>Also, its prompt acceptance technically seems to not be limited by anything.<br>\n",
        "\n",
        "**ChatGPT is a probabilistic system, a language model**.<br>\n",
        "**It continues a sequence started by our prompt by modeling a continuing sequence of words.**\n",
        "\n",
        "How does this work? What kind of model is applied under the hood?<br>\n",
        "[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) proposed the Transformer model architecture.<br>\n",
        "A transformer-based language model is a type of neural network architecture that is used for natural languages processing tasks such as language translation, text summarization, and language generation. The key innovation of the transformer architecture is the **attention mechanism**, which allows the model to weigh the importance of different parts of the input when making predictions.\n",
        "\n",
        "Transformers really took over the field of AI by now..."
      ],
      "metadata": {
        "id": "uoo37bwDGD2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "**We will train a transformer-based, character-level language model** on [Tiny-Shakespeare](https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt) (all of Shakespeare in a single file).\n",
        "\n",
        "Given a chunk of text from [Tiny Shakespeare](https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt), the transformer will decide on what character will follow.\n",
        "GPT is state-of-the-art (2022) in language modeling."
      ],
      "metadata": {
        "id": "KCKFIEqdZr_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HSklX08G76-",
        "outputId": "aca5faed-584b-4df8-9cd1-164dfa3b9e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "nZ3LC6LvZf0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset\n",
        "\n",
        "Let's first look at the contents of the dataset:"
      ],
      "metadata": {
        "id": "joKpVCV-e1zE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "5e0b6cbc-3e65-4182-990b-97b00e606abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 15:57:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-12 15:57:02 (15.1 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset - file input.txt\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: our small dataset contains Shakespeare texts contained into a file called `input.txt` of size ! MB. We are dealing with roughly 1 million characters. We will use this file to model how these characters follow each other."
      ],
      "metadata": {
        "id": "0Ad5mM9lamYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "b01405b4-1184-4b11-9d5e-4de0acc65afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "9b72cf92-0314-4020-bfd4-17cefe477232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))   # Get all unique characters in the text\n",
        "vocab_size = len(chars)           # Length of the vocabulary (this includes the space character)\n",
        "print(''.join(chars))             # joins all the characters in chars back into a single string\n",
        "print(f'vocabulary size', {vocab_size})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "d6efa76f-700d-40ef-c41d-2e7289296a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocabulary size {65}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: **The vocabulary size is a total of 65 characters for the text variable, apace included.**\n",
        "\n",
        "`chars = sorted(list(set(text)))` This line performs several operations on `text`:\n",
        "- `set(text)`: Converts the string text into a set of its unique characters. A set is a collection that automatically removes duplicates, so after this operation, each character from text will appear only once.\n",
        "- `list(set(text))`: Converts the set of unique characters back into a list. This is necessary because a set does not preserve order, and we might want to work with the characters in a specific sequence.\n",
        "- `sorted(list(set(text)))`: Sorts the list of unique characters. This ensures that the characters are in a consistent order, typically alphabetical for strings. The result is assigned to the variable chars.\n",
        "\n",
        "`print(''.join(chars))`: This line joins all the characters in chars back into a single string (with no spaces between them) and prints it. This shows what unique characters are present in the text, in sorted order."
      ],
      "metadata": {
        "id": "O3vJns4aTdnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Process - using the encoder and decoder"
      ],
      "metadata": {
        "id": "uh7MnG09q8nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers and vice-versa\n",
        "# building a look-up table via a dictionary\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # Character to index mapping\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # Index to character mapping\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]           # encode a string to a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decode a list of integers to a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "2d0e1c81-f107-4630-ac9a-78409f52abea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: this is a **very simple encoding/decoding procedure, in practice, people used subword units tokenizer**. Google, for example, uses a [sentencepiece](https://github.com/google/sentencepiece\n",
        ") schema for text tokenizer and detokenizer. SentencePiece implements **subword units** (e.g., **byte-pair-encoding (BPE)** meaning you are not coding entire words but you are not also encoding individual characters.  OpenAI has its library called [tiktoken](https://github.com/openai/tiktoken), it is a **fast BPE tokeniser** for use with OpenAI's models. It is efficient API usage. It helps developers estimate API usage costs by counting tokens in text and supports automatic loading for model-specific encoding."
      ],
      "metadata": {
        "id": "c9gfxH5Ek-60"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u5wGWwbZJAB"
      },
      "source": [
        "## Digression - Tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kYq-dIOZJAB"
      },
      "source": [
        "Different systems use different approaches to encoding/decoding.<br>\n",
        "For example, OpenAI uses byte-pair encoding (BPE) with their GPT-2 model.<br>\n",
        "BPE is a subword tokenization technique. It is a bit more complex than what we will do here, but its shown here nonetheless for a little bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y93h_Z2qZJAB",
        "outputId": "fbe09bda-75cd-4975-af23-f7f065bc5816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 4178, 612]\n",
            "hii there\n",
            "50257\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "msg = \"hii there\"\n",
        "token_list = enc.encode(msg)\n",
        "print(token_list) # BPE returns fewer tokens than the character encoding\n",
        "print(enc.decode(enc.encode(\"hii there\")))\n",
        "\n",
        "print(enc.n_vocab) # total amount of tokens in the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJ9oxl8ZJAB"
      },
      "source": [
        "NB: Tiktoken shows that there is a trade-off between the length of the encoding and the amount of tokens.<br>\n",
        "**We can have short sequences of tokens with very large vocabulary, or we can just as well have long sequences of tokens with a small vocabulary**.\n",
        "\n",
        "This BPE approach is used widely for NLP tasks nowadays."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the Entire Dataset"
      ],
      "metadata": {
        "id": "6TSuTS8XriCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f'Total size: {data.shape} elements of type {data.dtype}')\n",
        "print(data[:1000]) # the 1000 characters we looked at earlier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le6Xg9_hrqs7",
        "outputId": "f7950bb4-ad88-40c2-a94a-fadf924e7357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size: torch.Size([1115394]) elements of type torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: this output is the tokenization, character by character, of our 1000 characters text. For instance, `0` is a new line character and `1` is a space."
      ],
      "metadata": {
        "id": "8jnMYMGRs5o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Dataset\n",
        "\n",
        "The entire Tiny-Shakespeare text is now represented as a sequence of integers.\n",
        "We can start separating the data into training and validation sets.\n",
        "\n",
        "The **split** between a **training and a validation dataset** is a fundamental concept in machine learning and data science, aimed at creating robust and generalizable models. Here's an overview of what it means and why it's important:\n",
        "\n",
        "**Training Dataset**: This is the subset of the data that we use **to train our machine learning model**. The model learns to make predictions or decisions based on this data. The training process involves adjusting the model's parameters to minimize the error between the predicted outputs and the actual outcomes in the training dataset.\n",
        "\n",
        "**Validation Dataset**: This subset of the data is used **to evaluate the model's performance during the training phase**. It acts as a proxy for test data, helping to tune the hyperparameters (settings of the model that are fixed before the training process begins, like learning rate or the depth of a decision tree) and to prevent overfitting. Overfitting occurs when a model learns the training data too well, including its noise and outliers, making it perform poorly on unseen data because it has essentially memorized the training dataset rather than learned the underlying patterns.\n",
        "\n",
        "The primary purposes of creating a split between training and validation datasets include:\n",
        "\n",
        "**Model Evaluation**: The validation dataset provides a reliable estimate of the performance of the model on new, unseen data. This helps in evaluating how well the model has learned from the training dataset and how it generalizes to data it hasn't seen before.\n",
        "\n",
        "**Hyperparameter Tuning**: The validation set is crucial for tuning the model's hyperparameters. By evaluating the model's performance on the validation set, one can adjust the hyperparameters to find the best combination that maximizes the model's performance.\n",
        "\n",
        "**Preventing Overfitting**: Regularly checking the model's performance on the validation set during training can signal if the model is starting to memorize the training data rather than learning general patterns. If the model's performance on the training set improves while its performance on the validation set worsens, it's likely overfitting.\n",
        "\n",
        "A typical **workflow** involves iteratively training the model on the training dataset, assessing its performance on the validation dataset, and adjusting the model or its hyperparameters based on this assessment. After finalizing the model, its performance is then tested on a separate, untouched dataset known as the test dataset to evaluate its real-world applicability.\n",
        "\n",
        "The **split ratio** between the training and validation datasets can vary depending on the total size of the dataset and the specific problem or domain, but common splits include **70/30, 80/20**, or using techniques like k-fold cross-validation for more efficient use of the data."
      ],
      "metadata": {
        "id": "cdIQI6VXtsrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data between train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest 10% will be val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "PaPwEEkpurBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# amr\n",
        "print(f\"length training set:\", {len(train_data)})\n",
        "print(f\"length validation set:\", {len(val_data)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BuKGIPvhmp",
        "outputId": "4d0b509c-47c1-4762-824f-f40340012246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length training set: {1003854}\n",
            "length validation set: {111540}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block Size"
      ],
      "metadata": {
        "id": "53Xp-1g9_hfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now sample **random chunks out of the training set** and train them chunks at the time. Feeding the NN with the whole training set in one shot will not be computationally feasible and too expensive. **Chunks are basically the length of a sequence which is called a block size**.\n",
        "\n",
        "Block size in NLP tasks refers to the number of tokens (words or characters) that a model can process in a single input.\n",
        "\n",
        "For models like Transformers, the **block size is critical** because it determines the **sequence length that the model can handle at once**, affecting both the computational resources required and the model's ability to capture long-distance dependencies in the data."
      ],
      "metadata": {
        "id": "fgcgA_33wMqL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImXFvhoZJAC"
      },
      "source": [
        "Let's prepare the model. We will never feed our model the entire sequence of tokens as prompt at once.<br>\n",
        "Instead, we will feed it **a randomly drawn but consecutive sequence of tokens**.<br>\n",
        "The model will then predict the next token in the sequence from this prompt.<br>\n",
        "\n",
        "> We call these consecutive, size-limited input sequences of tokens **blocks**.<br>\n",
        "> Size-limited means that blocks can have a length of up to `block_size`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8              # Upper limit on the length of the text sequences\n",
        "train_data[:block_size+1]   # First 9 characters (8 + 1 for the target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex9Ii3aryveW",
        "outputId": "4888a57d-521e-4709-bbe0-1bb78783dc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: this is the first 9 characters in the sequence in the training set. When we plug this sequence into a Transformer, we are going to actually simultaneously train it to make prediction at every one. Now, **in this sequence of 9 characters, they are actually 8 individual examples**, the way it is processed is as follows:\n",
        "- in the context of [`18`], → `47` comes next\n",
        "- in the context of [`18, 47`], → `56` comes next\n",
        "- in the context of [`18,47,56`], → `58` comes next\n",
        "- ... and so on ...\n",
        "\n",
        "Let's spilled it out with code, see below."
      ],
      "metadata": {
        "id": "naNz1lqvzUPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the next token based on a given context\n",
        "\n",
        "# the first block of tokens\n",
        "x = train_data[:block_size] # x inputs to the Transformer, e.g. [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "# individual tokens shifted by one (also including the very last token now)\n",
        "y = train_data[1:block_size+1] # y is the target, # e.g. [2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# iterating over the block size of 8\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfjPExy-2UbW",
        "outputId": "c48da903-c126-4db5-cfab-f7740ba83e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: this way of processing makes the Transformer network used to seeing **contexts** all the way from as little as one, all the way to the block size. See code explained below:\n",
        "\n",
        "**Data preparation**:\n",
        "- `x` is defined as the first block_size tokens from `train_data`, serving as the initial context or input sequence for the model.\n",
        "- `y` is essentially `x` shifted by one position to the right, indicating the next token that should be predicted by the model given the context in `x`. The last token in `y` goes beyond the initial `block_size` tokens to include the immediate next token in the sequence.\n",
        "\n",
        "**Iterative Prediction Task Setup**:\n",
        "- The loop `for t in range(block_size)`: iterates through each position in the block_size, creating increasingly larger contexts and their corresponding targets.\n",
        "- `context = x[:t+1]` gradually increases the context window by including one more token from x in each iteration. Initially, the context contains just the first token, and by the end of the loop, it includes all block_size tokens.\n",
        "- `target = y[t]` identifies the next token that the model should predict based on the given context. It's the token immediately following the last token in the current context."
      ],
      "metadata": {
        "id": "DeKJQ7Tj7be7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Size\n",
        "Extracting a batch of sequences and the corresponding next elements as targets from these datasets.\n",
        "\n",
        "**The batch size is the number of training examples processed before the model's internal parameters are updated**. For example, if you have a dataset of 1000 sentences and you choose a batch size of 100, the dataset will be divided into 10 batches. Each batch of 100 sentences will be passed through the network in sequence, with each pass followed by an update to the model's weights.\n",
        "\n",
        "**Impact on Training:**\n",
        "\n",
        "**Memory Usage**: A larger batch size requires more memory, as more data needs to be loaded and processed simultaneously. This can be a limiting factor depending on the hardware being used for training.\n",
        "**Convergence**: The choice of batch size can affect how quickly and smoothly the model converges to a solution. **Smaller batches** often lead to faster convergence but can result in a more erratic learning process. **Larger batches** provide more stable and accurate estimates of the gradient, but they might make the learning process slower and potentially get stuck in local minima.\n",
        "> **Generalization**: Some studies suggest that smaller batch sizes may lead to better generalization in the trained model. This is thought to be because the noise introduced by the smaller subsets helps to regularize the model.\n",
        "\n",
        "**Types of Batch Size**:\n",
        "\n",
        "- **Mini-Batch Gradient Descent**: This is the most common training method, where the batch size is a compromise between the extremes of 1 example per batch (stochastic gradient descent) and the entire dataset per batch (batch gradient descent). It balances the need for computational efficiency with the benefits of stochastic updates.\n",
        "- **Choosing Batch Size**: The optimal batch size is often determined experimentally, as it can depend on the specific task, the model architecture, and the hardware capabilities. Researchers and practitioners might start with a value that fits their system’s memory constraints and adjust based on training speed and model performance outcomes.\n",
        "\n",
        "In summary, batch size in NLP tasks (and machine learning more broadly) is a critical hyperparameter that influences the efficiency, convergence speed, and generalization performance of the training process."
      ],
      "metadata": {
        "id": "hdDnMNyk_lzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "\n",
        "Every time we are going to feed inputs to the transformer, we are going to have **many batches of multiple chunks of text that are stacked up in a single tensor**. It is done for efficiency as GPUs are very good at the parallel processing of data. The 1-dimensional arrays are going to be stacked up to form a 4×8 tensor, that is with a sequence length (also called block size or context) of 4 and a batch size of 8."
      ],
      "metadata": {
        "id": "1PJ3T9qSFSTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a batch of data of sequence x and target y\n",
        "\n",
        "torch.manual_seed(1337) # set the random number generator seed to a fixed value, i.e. 1337; important for reproducibility\n",
        "batch_size = 8  # number of sequences in a batch / processed in parallel\n",
        "block_size = 4  # maximum sequence length serving as a context/prompt\n",
        "\n",
        "def get_batch(split):\n",
        "    # Generate a batch of inputs/prompts x and respective targets y\n",
        "    # batches are always of shape (batch_size, block_size)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Tensor of shape (batch_size) with random sequence start indices between 0 and len(data) - block_size\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Accumulate and add each sequence of this batch to form a tensor (tensor shape: batch size, block size)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\n",
        "    # Same as x but shifted by one token\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets\n",
        "    return x, y # x is (4,8), y is (4,8) too\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('------')\n",
        "\n",
        "for b in range(batch_size):      # batch dimension, number of sequences in the batch (batch_size)\n",
        "    for t in range(block_size):  # time dimension, number of tokens in the sequence  (block_size)\n",
        "        context = xb[b, :t+1]    # context means prompt, taking the first t+1 tokens from the b-th sequence in the batch\n",
        "        target = yb[b, t]        # we take the t-th token from the b-th sequence in the batch for the target (the token we want to predict)\n",
        "        print(f\"when input is {context.tolist()} the target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT6Hfcf6Ah94",
        "outputId": "ea3a096a-9fb5-4fba-9e3a-22e4e7af2f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([8, 4])\n",
            "tensor([[ 1, 60, 39, 47],\n",
            "        [46, 43, 39, 60],\n",
            "        [ 1, 46, 43, 56],\n",
            "        [61, 47, 50, 50],\n",
            "        [43,  1, 39, 52],\n",
            "        [53, 58, 46,  1],\n",
            "        [53,  1, 40, 43],\n",
            "        [ 1, 56, 43, 45]])\n",
            "targets:\n",
            "torch.Size([8, 4])\n",
            "tensor([[60, 39, 47, 50],\n",
            "        [43, 39, 60, 43],\n",
            "        [46, 43, 56, 43],\n",
            "        [47, 50, 50,  1],\n",
            "        [ 1, 39, 52,  1],\n",
            "        [58, 46,  1, 40],\n",
            "        [ 1, 40, 43,  1],\n",
            "        [56, 43, 45, 39]])\n",
            "------\n",
            "when input is [1] the target is 60\n",
            "when input is [1, 60] the target is 39\n",
            "when input is [1, 60, 39] the target is 47\n",
            "when input is [1, 60, 39, 47] the target is 50\n",
            "when input is [46] the target is 43\n",
            "when input is [46, 43] the target is 39\n",
            "when input is [46, 43, 39] the target is 60\n",
            "when input is [46, 43, 39, 60] the target is 43\n",
            "when input is [1] the target is 46\n",
            "when input is [1, 46] the target is 43\n",
            "when input is [1, 46, 43] the target is 56\n",
            "when input is [1, 46, 43, 56] the target is 43\n",
            "when input is [61] the target is 47\n",
            "when input is [61, 47] the target is 50\n",
            "when input is [61, 47, 50] the target is 50\n",
            "when input is [61, 47, 50, 50] the target is 1\n",
            "when input is [43] the target is 1\n",
            "when input is [43, 1] the target is 39\n",
            "when input is [43, 1, 39] the target is 52\n",
            "when input is [43, 1, 39, 52] the target is 1\n",
            "when input is [53] the target is 58\n",
            "when input is [53, 58] the target is 46\n",
            "when input is [53, 58, 46] the target is 1\n",
            "when input is [53, 58, 46, 1] the target is 40\n",
            "when input is [53] the target is 1\n",
            "when input is [53, 1] the target is 40\n",
            "when input is [53, 1, 40] the target is 43\n",
            "when input is [53, 1, 40, 43] the target is 1\n",
            "when input is [1] the target is 56\n",
            "when input is [1, 56] the target is 43\n",
            "when input is [1, 56, 43] the target is 45\n",
            "when input is [1, 56, 43, 45] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our batch of inputs to feed to the transformer (tensor shape: (B,T) = batch size 8, block size 4)\n",
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "786c2eb9-9dff-45fa-d309-60ce9d5c6eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 60, 39, 47],\n",
            "        [46, 43, 39, 60],\n",
            "        [ 1, 46, 43, 56],\n",
            "        [61, 47, 50, 50],\n",
            "        [43,  1, 39, 52],\n",
            "        [53, 58, 46,  1],\n",
            "        [53,  1, 40, 43],\n",
            "        [ 1, 56, 43, 45]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amr\n",
        "# checking one row from the batch input\n",
        "print(xb[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bFOZqE0UW0-",
        "outputId": "b39cf5c6-362b-483c-8c96-badddd71b8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 60, 39, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NB: function `get_batch` code explained below:\n",
        "\n",
        "`ix = torch.randint(len(data) - block_size, (batch_size,))`\n",
        "This line generates a **tensor of random integers** `ix` using PyTorch's randint function. The integers are in the range [0, len(data) - block_size), which means **starting from 0 up to the length of the selected data minus block_size**. The size of the tensor is determined by batch_size, which means it will contain batch_size random integers. **These integers are used as the starting indices for the batches of data to be extracted**.\n",
        "\n",
        "`x = torch.stack([data[i:i+block_size] for i in ix])`\n",
        "This line uses a list comprehension to iterate over each starting index in ix and slices the data tensor from that index i to i + block_size, creating a sequence of data. `torch.stack` then combines these sequences into a new tensor x, where each sequence is a separate element in the batch. **This tensor x represents the input data for the model**.\n",
        "\n",
        "`y = torch.stack([data[i+1:i+block_size+1] for i in ix])`\n",
        "This line is similar to the previous one but creates the target tensor y. For each starting index i in ix, it slices the data tensor from i + 1 to i + block_size + 1. **This represents the target or \"next\" elements corresponding to the inputs in x**."
      ],
      "metadata": {
        "id": "xPzXpwRNJQsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# amr testing the code above\n",
        "# generating a tensor with random sequence (tensor shape: batch size)\n",
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "print(ix)\n",
        "print(ix.shape)\n",
        "\n",
        "# stacking the sequences (tensor shape: batch size, block size)\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "print(x)\n",
        "print(x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alr2NLZf-lqk",
        "outputId": "c71b7231-4d45-4f22-dcfe-53acfdd7d12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([400784, 110140, 944762, 354070, 724297, 412236, 176790, 256488])\n",
            "torch.Size([8])\n",
            "tensor([[56, 57, 46,  1],\n",
            "        [46, 53, 59,  1],\n",
            "        [53, 51, 44, 53],\n",
            "        [43, 56,  5, 57],\n",
            "        [58, 10,  1, 58],\n",
            "        [47, 52, 45,  1],\n",
            "        [51, 54, 50, 39],\n",
            "        [ 1, 53, 44,  1]])\n",
            "torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paA-6Qm9NHwX"
      },
      "source": [
        "## Embedding Layer\n",
        "\n",
        "An input batch consists of tensors `xb` and `yb`.<br>\n",
        "Both `xb` and `yb` are of size $batch\\_size \\times block\\_size$.\n",
        "\n",
        "The batch is used as basis for 'sub-batching'.\n",
        "\n",
        "Because `yb` is just `xb` shifted by one token, we can use `yb` to train<br>\n",
        "on multiple examples *within a batches' partial sequences*, each being of different context size.\n",
        "\n",
        "These 'sub-batches' are called `context` and `target`. They are the pairs that we will feed into the model.\n",
        "\n",
        "For now, we can start focusing on the model itself and feed `xb` and later on `yb` into it.<br>\n",
        "\n",
        "We'll start building a bigram model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a simple bi-gram model\n",
        "A bi-gram model, in NLP, is a type of statistical language model that predicts the probability of a word given the preceding word. **It's called a bi-gram because it considers a \"gram\" (or token) in the context of one preceding gram, thus forming pairs or \"bi\"-grams**.\n",
        "\n",
        "**Example**: If your corpus had the sentence \"*The quick brown fox jumps*\", the bi-grams would be: \"The quick\", \"quick brown\", \"brown fox\", and \"fox jumps\". A bi-gram model would use these to calculate the likelihood of \"brown\" following \"quick\", \"fox\" following \"brown\", and so forth.\n",
        "\n",
        "In summary, a bi-gram model is a simple language model that can predict the next unit in a sequence based on the preceding one and is often used in applications requiring a balance between contextual relevance and computational simplicity.\n"
      ],
      "metadata": {
        "id": "XKj9t31yNAey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a model that predicts the next token based on the previous token:\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Embedding the vocabulary\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)   # 65 embeddings (vocabulary), embedding size: 65-dim vectors\n",
        "\n",
        "    def forward(self, idx, targets=None):       # if targets not provided, the method computes no loss\n",
        "        # idx and targets are both (B,T) tensor of integers (batch_size, block_size)\n",
        "        # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape        # B = batch_size, T = block_size, C = vocab_size\n",
        "            logits = logits.view(B*T, C)  # Transpose logits to (B*T, C)\n",
        "            # This is the first time we actively use the targets:\n",
        "            targets = targets.view(B*T)   # Transpose targets to (B*T) (targets contains the next token's index for each input sequence in the batch)\n",
        "            loss = F.cross_entropy(logits, targets)  # Calculating cross entropy loss across all tokens in the batch (using targets to plug out the correct token for each input sequence)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Instantiate the model\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)              # Forward pass (yb remains unused for now)\n",
        "print(f\"logit shape\", {logits.shape}) # [B,T], vocabulary_size)\n",
        "print(f\"loss shape\", {loss})\n",
        "print(f\"loss\", {loss})\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "ad8a2074-1b29-418e-ffca-1dabacc1a80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logit shape {torch.Size([32, 65])}\n",
            "loss shape {tensor(4.7724, grad_fn=<NllLossBackward0>)}\n",
            "loss {tensor(4.7724, grad_fn=<NllLossBackward0>)}\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: the text generation is garbage! because it is a totally random model, the token are not interconnected."
      ],
      "metadata": {
        "id": "sgoFEaie91vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Every integer of our tokenized text is now represented by an embedding vector of size `vocab_size`**.<br>\n",
        "We do this by using **an embedding layer**. **This layer is effectively a lookup table that maps<br>**\n",
        "each possible (`vocab_size` are possible in total) character-representing index to a unique vector of size `vocab_size`.\n",
        "\n",
        "**The `logits` are the outputs of the model.<br>**\n",
        "We just treat the embedded tokens of the input batch as the logits.<br>\n",
        "This `logits` tensor holds all the embedded identities of the tokens in the input batch -> ($batch\\_size \\times block\\_size \\times vocab\\_size$).\n",
        "\n",
        "> We are *not yet* interconnecting the tokens with any sort of model/logic.<br>\n",
        "We are not yet training or predicting *anything*.\n",
        "\n",
        "This is about to change."
      ],
      "metadata": {
        "id": "lVN1E6aiJg2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NB: Code Explaination for the above forward pass and text generation:\n",
        "\n",
        "**Forward Pass**: `forward(self, idx, targets=None)`\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `idx`: A tensor of shape `(B, T)` containing indices of tokens, where `B` is the batch size and `T` is the sequence length.\n",
        "- `targets`: A tensor of shape `(B, T)` containing the indices of the target tokens. If targets is not provided, the method computes no loss.\n",
        "\n",
        "**Process**:\n",
        "\n",
        "- The method retrieves the logits for each token in `idx` using the token_embedding_table.\n",
        "- If `targets` is provided, it reshapes the logits to `(B*T, C)` and the targets to `(B*T)`, then computes the cross-entropy loss between the logits and the targets. This loss measures how well the model predicts the next token.\n",
        "- Returns: The logits and the computed loss (if `targets` is provided; otherwise, `loss` is `None`).\n",
        "\n",
        "**Text Generation**: `generate(self, idx, max_new_tokens)`\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `idx`: A tensor of shape `(B, T)` representing the initial context for generation.\n",
        "- `max_new_tokens`: The maximum number of new tokens to generate.\n",
        "\n",
        "**Process**:\n",
        "\n",
        "- The method iteratively generates one token at a time based on the current context (idx).\n",
        "- It updates the context by appending the newly generated token and repeats the process until `max_new_tokens` have been generated.\n",
        "- For each new token, it computes the logits for the last token in the current context, applies softmax to convert these logits into probabilities, and samples a new token index from this probability distribution.\n",
        "- Returns: The tensor `idx` containing the original context plus the newly generated tokens.\n",
        "\n",
        "**Model Instantiation and Usage**\n",
        "\n",
        "- An instance of `BigramLanguageModel` is created with a specified vocab_size.\n",
        "- The model is then used to compute logits and loss for a given batch of inputs `(xb, yb)` and to generate text starting from an initial context.\n",
        "\n",
        "**Function Calls and Outputs**\n",
        "\n",
        "- `logits.shape` and `loss`: Prints the shape of the logits tensor and the value of the loss computed during the forward pass.\n",
        "- decode(...): Assuming the decode function maps token indices back to their string representations, this line generates 100 new tokens starting from an initial context of a single zero index and prints the decoded text.\n",
        "\n",
        "This model is a simplistic representation of language modeling and is more illustrative than practical, especially since the embedding table is used in an unconventional way to directly produce logits for next-token prediction."
      ],
      "metadata": {
        "id": "7qR8NgBXcg2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# amr\n",
        "# looking at the model\n",
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDOtDRjtW3LL",
        "outputId": "120dd319-0dcf-4f82-dfc9-1c2e59619159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 65)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amr\n",
        "# Access the embeddings weight\n",
        "embeddings_weight = m.token_embedding_table.weight.data\n",
        "\n",
        "print(\"Shape:\", embeddings_weight.shape)\n",
        "print(\"Embeddings weights:\", embeddings_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMo_g18IFvR5",
        "outputId": "547582ce-bf36-4b44-8c4b-725c82f3ae10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([65, 65])\n",
            "Embeddings weights: tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
            "        ...,\n",
            "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
            "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amr\n",
        "# Token index to check\n",
        "token_index = 5  # Replace with the index of the token you want to check\n",
        "\n",
        "# Get the embeddings for the token at the specified index\n",
        "token_embeddings = embeddings_weight[token_index]\n",
        "\n",
        "print(\"Embeddings for token at index\", token_index, \":\", token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE8lj4nLGO5O",
        "outputId": "67caf637-3eeb-4790-b059-196dbb36e1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for token at index 5 : tensor([-0.1338,  0.3899, -0.2884, -1.4651,  0.0101, -0.3004, -1.5733,  0.0148,\n",
            "        -0.0447, -0.5367, -0.5223, -0.2181, -2.1608,  0.7865,  0.6854, -1.2576,\n",
            "         0.6094, -2.0551, -0.4431, -0.6499, -0.6870,  0.2567, -1.2669,  0.2645,\n",
            "        -0.6445,  1.0834, -0.7995,  0.2922,  1.3143,  1.2607, -0.3505, -2.0660,\n",
            "         1.0575, -1.0572,  0.9911, -0.0797,  1.0751,  0.2381,  0.5757,  1.6685,\n",
            "         0.5976, -1.8736,  1.2910, -0.3753, -1.8943,  0.5557,  0.8567, -0.8461,\n",
            "         0.5015, -0.9656, -0.7255,  0.0990,  0.5928, -0.0422, -0.9566,  1.4424,\n",
            "         0.4341, -0.4292,  0.3666,  0.1275, -0.0560,  0.8315, -0.5512,  1.0477,\n",
            "         1.6187])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the optimizer"
      ],
      "metadata": {
        "id": "TT3L59FztojH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up a Loss Function"
      ],
      "metadata": {
        "id": "iD8QpJKPtlg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic Pytorch training loop\n",
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    # The model m processes the batch xb and compares the predictions (logits) against the actual targets yb to compute the loss,\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # argument is an optimization that can potentially speed up gradient zeroing.\n",
        "    loss.backward()                       # Computes the gradient of the loss\n",
        "    optimizer.step()                      # Updates the model parameters based on the computed gradients and the optimizer algorithm\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e362fd-12ca-41bf-c22a-89ad2c2fecfc",
        "id": "96G0TS6YY0hG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6548352241516113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "32b0e865-9ac1-4759-f618-3ba127278b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LORD RIZEREL:\n",
            "Neave fling to: yet spit this wrong way.\n",
            "\n",
            "ISABELA:\n",
            "Share is thou? There thou bettague!\n",
            "\n",
            "Nurself.\n",
            "\n",
            "PENE:\n",
            "Why mine arsought; I I day with moss!\n",
            "\n",
            "CORIOLANUS:\n",
            "O, comple you wither titted to\n",
            "prayison of shall prisone in evass!-\n",
            "Nopper'd compatter! May, say, what cames Lanward't, so say lord.\n",
            "\n",
            "FROMEUSS:\n",
            "Heneme, what fear in the frame but say\n",
            "Yorshy of that, if now love; if in a passsians thy new,\n",
            "I in that may didspoken to amble and lespere straid's sake ling mind houme of of spirit,\n",
            "My \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: the output is much better! more like Shakespeare writings!"
      ],
      "metadata": {
        "id": "SeZ3eZL8agqy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JU8omhOuD1W"
      },
      "source": [
        "Given that we have the identities of the next character through `yb`, how well does the model predict them through the `logits`? **The `loss` is the measurement of prediction quality**.\n",
        "\n",
        "We want the index within `yb` to be the same as the most likely/active index within `logits`.<br>\n",
        "The loss is measured as the average of this across all the tokens in the input batch.\n",
        "\n",
        "We know the `vocab_size` is $65$.<br>\n",
        "We can calculate what the loss should be if we were to predict the next token totally randomly:\n",
        "\n",
        "$$-ln(\\frac{1}{65}) = 4.1743872699$$\n",
        "\n",
        "**Our calculated loss is higher/worse, because we are not predicting perfectly randomly to begin with.<br>**\n",
        "**The initial predictions are not perfectly spread out across the `vocab_size`.<br>**\n",
        "**They aren't super diffuse and contain a bit of entropy.<br>**\n",
        "**We haven't yet learned uniform distribution across the `vocab_size`**.\n",
        "\n",
        "![](https://images.squarespace-cdn.com/content/56316c94e4b098620a45e78a/1457973972468-D5XJVA1ABFXSD0AH9RZC/?content-type=image%2Fpng)\n",
        "<br>Source: [Shiken](https://shiken.ai/chemistry/entropy)\n",
        "\n",
        "The `loss` is to be minimized.<br>\n",
        "We will need the model to make predictions of individual next tokens.<br>\n",
        "\n",
        "Let's append the current model with a function `generate` that takes in the last token of a sequence and returns the next token however many times we want:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amr - test section\n",
        "# the code below at token 0 will give a line return\n",
        "idx = torch.zeros(1,1); idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ68oSrDo_Jo",
        "outputId": "4398a150-8aac-48c7-e576-ddaea2821a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NB: code explained for the text generation, see below:\n",
        "\n",
        "Generation setup\n",
        "\n",
        "- `m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)`: This line generates a sequence of 500 new tokens starting from an initial context provided by `idx`. **Here, `idx` is initialized as a tensor of zeros with shape (1, 1), implying that the generation starts with no specific context, actually token 0 will produce a line return**. The model `m` likely uses a form of prediction mechanism (such as sampling or greedy selection) based on the learned distributions to choose the next token at each step.\n",
        "\n",
        "Token list conversion\n",
        "\n",
        "- `[0].tolist()`: The generated sequence is accessed (assuming the model may return a batch of sequences, and we're interested in the first) and converted to a Python list of token indices."
      ],
      "metadata": {
        "id": "4hrJaz_noQoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention\n",
        "- matrix multiplication\n",
        "- applying a mask\n",
        "- attention mechanism"
      ],
      "metadata": {
        "id": "uAKfOKUiJ-r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "# This sets the seed for the random number generator for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# tensor a (3x3) - triangular matrix\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "print(\"Matrix a orig = \")\n",
        "print(a)\n",
        "print('--')\n",
        "\n",
        "# tensor a: normalizing  each row of the tensor by dividing it by the sum of its elements,\n",
        "# computes the sum of elements along the columns (dimension 1), and\n",
        "# keepdim=True keeps the dimension for broadcasting to work correctly during the division.\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "# tensor b (2 x 3) of random integers between 0 and 9, from a uniform distribution\n",
        "# The .float() method converts the tensor from an integer type to a floating-point type, necessary for subsequent matrix operations\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "# Pytorch matrix multiplication\n",
        "c = a @ b\n",
        "\n",
        "# printing matrices\n",
        "print('Matrix a normalized =')\n",
        "print(a)\n",
        "print('--')\n",
        "print('Matrix b =')\n",
        "print(b)\n",
        "print('--')\n",
        "print('Matrix c = matmul a@b')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e23c31-c89a-455d-ebb6-7a26e087b1f5",
        "id": "bTSzuZvngQtF"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix a orig = \n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "Matrix a normalized =\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "Matrix b =\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "Matrix c = matmul a@b\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: matrices summary ;\n",
        "- Matrix a: A lower triangular matrix with normalized rows.\n",
        "- Matrix b: A 3x2 matrix of random floating-point numbers between 0 and 9.\n",
        "- Matrix c: The result of the matrix multiplication of a with b, a 3x2 matrix.\n",
        "\n",
        "\n",
        "> **Matrix c**: the first row of c is identical to the first row of b because the first row of a is [1, 0, 0], meaning it multiplies the first element of b by 1 and adds zeros for the rest.\n",
        "The second row of c is the average of the first and second rows of b because the second row of a is [0.5, 0.5, 0].\n",
        "The third row of c is the average of all three rows of b because the third row of a is [0.3333, 0.3333, 0.3333]."
      ],
      "metadata": {
        "id": "jhIFsP3Bj4tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337) # set the seed for reproducibility\n",
        "B,T,C = 4,8,2           # batch, time, channels\n",
        "x = torch.randn(B,T,C)  # random numbers for a tensor of shape B,T,C\n",
        "print(x)\n",
        "print(f\"tensor x shape \\n\", {x.shape})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfLNmQgRJ-r4",
        "outputId": "8f512945-207c-4a4b-da47-49e2b317e9ad"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n",
            "tensor x shape \n",
            " {torch.Size([4, 8, 2])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: x can represent an input tensor to a neural network where you have a batch of 4 sequences, each sequence of length 8, and each element of the sequence has 2 channels (features)."
      ],
      "metadata": {
        "id": "1O9IO2LgpZ1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, here we have  8  tokens, each of which is a vector of size  2 .\n",
        "They are not talking to each other / are not related to each other in any way.\n",
        "\n",
        "We'd like to couple them so that e.g. the 3rd token can only communicate with the tokens in the 2nd and 1st location, but not with a future token in the 4th location.\n",
        "\n",
        "Information has to be able to flow, but exclusively in one direction.\n",
        "\n",
        "We can do this in a most simple way by averaging preceding tokens, including the current_token. This would, in essence, summarize current_token in the context of current_token's history.\n",
        "\n",
        "For every  𝑡 -th token, we'd like to get the average of all the vectors of previous tokens and the current one ( 𝑡 ) as well:"
      ],
      "metadata": {
        "id": "Jqhq4-kJp5_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "E_9pvaV5ZJAR",
        "outputId": "f2694123-79b9-4a0d-ff15-26ab53c4e5e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "# We want x[b, t] = mean_{i <= t} x[b, i]\n",
        "xbow = torch.zeros((B, T, C))          # Create tensor of zeros of shape (B, T, C) (bag of words representation of the input)\n",
        "for b in range(B):                     # For all batches\n",
        "    for t in range(T):                 # For all tokens in the batch\n",
        "        xprev = x[b, :t+1]             # Get all tokens up to and including the current token (t, C)\n",
        "        xbow[b, t] = xprev.mean(dim=0) # Calculate the mean of the tokens up to and including the current token\n",
        "\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")     # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow[0]) # Running averages of the first batch of 8 tokens, each of size 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAXaGKJOZJAR"
      },
      "source": [
        "NB: Due to the loops, this is relatively inefficient.<br>**The trick is that we can build a running average like this using<br>\n",
        "much faster matrix multiplication:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: it uses a weighted sum approach, leveraging matrix multiplication for efficiency.\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x                         # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")       # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow2[0])  # Running averages of the first batch of 8 tokens, each of size 2\n",
        "\n",
        "# comparing the bag of words (xbow, xbow2)\n",
        "# Set a higher absolute tolerance\n",
        "atol_value = 1e-6  # For example, you can adjust this value as needed\n",
        "\n",
        "# Use the torch.allclose function with the custom atol\n",
        "close = torch.allclose(xbow, xbow2, atol=atol_value)\n",
        "print(close)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc00a067-1ae7-40c0-f490-d839516327e6",
        "id": "ZzmEqXTIKbtz"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: it uses the Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")       # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow3[0])  # Running averages of the first batch of 8 tokens, each of size 2\n",
        "\n",
        "# comparing the bag of words (xbow, xbow2)\n",
        "# Set a higher absolute tolerance\n",
        "atol_value = 1e-6  # For example, you can adjust this value as needed\n",
        "\n",
        "# Use the torch.allclose function with the custom atol\n",
        "close = torch.allclose(xbow, xbow3, atol=atol_value)\n",
        "print(close)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwSlBlSfJ-r4",
        "outputId": "8d813a8e-07de-4036-913f-7396d38a48a9"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "NS0HxWZwJ-r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9892d9-a65a-4847-9d1c-9e04a40d3060"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "id": "sq1XihWQJ-r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b399a33-e30f-48ea-cb54-9ffda36e8fd9"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "Yn4yHPkgJ-r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "fCSXaWP5J-r5"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "id": "e3RTps1mJ-r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef176122-f522-4498-c75b-c5d9ab7ceb9f"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "id": "vPM0sJYkJ-r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34aff066-39ff-43c7-eb89-60f64f86624a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "id": "wkJa1VaRJ-r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d180e5b-f7f1-4b47-8591-05a8a88b8126"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "id": "ODbO4KR0J-r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb6b331-487b-43d7-e122-86ba794a11a8"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "id": "X_lvQfrDJ-r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee44540-0aa6-4dcc-d818-e83f32013079"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "ij5lbJ4SJ-r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a20699-c856-4155-f6c9-bf6d7215cdc8"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "id": "06k0lpplJ-r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaae9c55-6120-49a8-85ff-9b7b04b158b6"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "id": "_o7Tno5DJ-r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518272ab-4692-4186-873f-e3dea1b79db5"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dM17Z7hAJ-r7"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "OTdl1JFmJ-r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "yvYgaVvHJ-r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63ad84c-cf5f-4798-ac37-67073fecf301"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5060\n",
            "step 300: train loss 2.4199, val loss 2.4337\n",
            "step 400: train loss 2.3500, val loss 2.3563\n",
            "step 500: train loss 2.2961, val loss 2.3126\n",
            "step 600: train loss 2.2408, val loss 2.2501\n",
            "step 700: train loss 2.2053, val loss 2.2187\n",
            "step 800: train loss 2.1636, val loss 2.1870\n",
            "step 900: train loss 2.1226, val loss 2.1483\n",
            "step 1000: train loss 2.1017, val loss 2.1283\n",
            "step 1100: train loss 2.0683, val loss 2.1174\n",
            "step 1200: train loss 2.0376, val loss 2.0798\n",
            "step 1300: train loss 2.0256, val loss 2.0645\n",
            "step 1400: train loss 1.9919, val loss 2.0362\n",
            "step 1500: train loss 1.9696, val loss 2.0304\n",
            "step 1600: train loss 1.9625, val loss 2.0470\n",
            "step 1700: train loss 1.9402, val loss 2.0119\n",
            "step 1800: train loss 1.9085, val loss 1.9957\n",
            "step 1900: train loss 1.9080, val loss 1.9869\n",
            "step 2000: train loss 1.8834, val loss 1.9941\n",
            "step 2100: train loss 1.8727, val loss 1.9758\n",
            "step 2200: train loss 1.8585, val loss 1.9622\n",
            "step 2300: train loss 1.8537, val loss 1.9503\n",
            "step 2400: train loss 1.8419, val loss 1.9424\n",
            "step 2500: train loss 1.8153, val loss 1.9407\n",
            "step 2600: train loss 1.8267, val loss 1.9374\n",
            "step 2700: train loss 1.8126, val loss 1.9344\n",
            "step 2800: train loss 1.8054, val loss 1.9230\n",
            "step 2900: train loss 1.8045, val loss 1.9339\n",
            "step 3000: train loss 1.7963, val loss 1.9243\n",
            "step 3100: train loss 1.7691, val loss 1.9208\n",
            "step 3200: train loss 1.7506, val loss 1.9092\n",
            "step 3300: train loss 1.7548, val loss 1.9038\n",
            "step 3400: train loss 1.7582, val loss 1.8960\n",
            "step 3500: train loss 1.7376, val loss 1.8934\n",
            "step 3600: train loss 1.7232, val loss 1.8888\n",
            "step 3700: train loss 1.7280, val loss 1.8814\n",
            "step 3800: train loss 1.7221, val loss 1.8951\n",
            "step 3900: train loss 1.7228, val loss 1.8789\n",
            "step 4000: train loss 1.7168, val loss 1.8635\n",
            "step 4100: train loss 1.7168, val loss 1.8798\n",
            "step 4200: train loss 1.7088, val loss 1.8672\n",
            "step 4300: train loss 1.6995, val loss 1.8501\n",
            "step 4400: train loss 1.7096, val loss 1.8686\n",
            "step 4500: train loss 1.6907, val loss 1.8546\n",
            "step 4600: train loss 1.6868, val loss 1.8348\n",
            "step 4700: train loss 1.6786, val loss 1.8346\n",
            "step 4800: train loss 1.6659, val loss 1.8445\n",
            "step 4900: train loss 1.6711, val loss 1.8384\n",
            "step 4999: train loss 1.6630, val loss 1.8230\n",
            "\n",
            "ROMEO:\n",
            "But you far you\n",
            "my swap with thus; come hath I uD\n",
            "If sleemition of where's granded\n",
            "Of their of tout the gortune upwon alond, liege man to is Iell this surpe\n",
            "And than sleue thus mind, his by blow,\n",
            "Virdty toward butied, Ditire spresiss with thou some not.\n",
            "\n",
            "LORIO:\n",
            "I am part\n",
            "But thou sging them but\n",
            "shat secondes morry thou sovore.\n",
            "\n",
            "ISABUS:\n",
            "What art sade but hither, thange e'en,\n",
            "Protes as kingle me; an your tords whom are Ineal.\n",
            "\n",
            "MENENIUS:\n",
            "But little sweet, hom, foust cerfort;\n",
            "Winth hing diend enirs' tompy beds sick ways!\n",
            "What curforself this grace. Won, passes us.\n",
            "\n",
            "BUCKINGHABY MARD:\n",
            "Mether star: keep it any head which\n",
            "He tall devioly that, out that confer old.\n",
            "Our thy dears time.\n",
            "Nay, the fragoly, pair, of new\n",
            "my father, my lip Backnoward:\n",
            "God therring for respide\n",
            "What colvery, teminelyord, I mast,\n",
            "While us that such differs I'll that confect I come,\n",
            "But; man.\n",
            "\n",
            "VOLUMNIO:\n",
            "Ontread confail with me. Humser dipporbried answeraw is codal one,\n",
            "Onjestion, not or cheavess ensty with.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HENRY Mess to Lies?\n",
            "Stand and these beguare youf stile that than war\n",
            "offity are, I usquesch\n",
            "Frown movhapty not duke with you addom\n",
            "grack prowd--lost\n",
            "But but they worse is senst my crunne undolier. But, beauts pruntaly; I stoll'ct my nor Murder, I sot, though who speak\n",
            "Your bout told-man rathing if anyshal\n",
            "epitence, tirre no the said he's,\n",
            "Andis frultifs. what his lide? That mirdy this dudgetions?\n",
            "\n",
            "KING ARINIA:\n",
            "I let holt not sucKether,\n",
            "Whither, efore But lord: I, beget because at that his say\n",
            "as to brought grave a donesmer all nobe.\n",
            "\n",
            "BUCKINGHUMBY:\n",
            "Which forgeled! Came; nor thereforn's fiends strefet.\n",
            "\n",
            "PLORIA:\n",
            "Yet to Capprohning, that brird\n",
            "of say mover a desrick.\n",
            "\n",
            "MO\n",
            "stompars, God the\n",
            "citchard is high.\n",
            "\n",
            "Seth Second Methere:\n",
            "Marrmat I unmale the bretcius unfoect that I would back where own thy lurges\n",
            "And, iffillimorture:\n",
            "As thou twand, York these that high praut.\n",
            "Plafe merprates sure dread with her,\n",
            "At not your must I suchon? too prant!\n",
            "O 'hiles clight the bleave is graved before\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14upF8cuJ-r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULL CODE EXPLANATION\n"
      ],
      "metadata": {
        "id": "k_Nvn2pJJSSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters:\n",
        "\n",
        "```\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "```\n",
        "\n",
        "\n",
        "- `batch_size = 16`: The number of sequences processed in parallel during training. Batch size affects both learning dynamics and computational efficiency.\n",
        "- `block_size = 32`: The maximum sequence length the model will consider for predictions. In the context of Transformers, this is often referred to as the model's \"context size\" or \"sequence length,\" limiting how much prior context the model can use.\n",
        "- `max_iters = 5000`: The maximum number of iterations (or updates) to perform during training. This caps the training process and is crucial for preventing overfitting and unnecessary computation.\n",
        "- `eval_interval = 100`: The interval (in iterations) at which the model's performance is evaluated on a validation set or some evaluation metric is calculated. This helps in monitoring the training progress.\n",
        "- `learning_rate = 1e-3`: The step size used for updating the model's weights during optimization. The learning rate is a critical hyperparameter that affects training stability and convergence.\n",
        "- `device = 'cuda' if torch.cuda.is_available() else 'cpu'`: Determines the computing device for training. If CUDA (NVIDIA's GPU computing platform) is available, training will be accelerated using the GPU; otherwise, it falls back to the CPU.\n",
        "- `eval_iters = 200`: This parameter might indicate the number of iterations to run during each evaluation phase, but its specific role depends on the context in which it's used in the training or evaluation loop.\n",
        "- `n_embd = 64`: The size of the embeddings used for representing tokens. This could also imply the dimensionality of the model's hidden layers.\n",
        "- `n_head = 4`: The number of attention heads in each Transformer block. Multi-head attention allows the model to focus on different parts of the input sequence simultaneously.\n",
        "- `n_layer = 4`: The number of layers (or depth) of the Transformer model. More layers can increase the model's capacity but also its computational cost and the risk of overfitting.\n",
        "- `dropout = 0.0`: The dropout rate used for regularization during training. Dropout randomly zeroes some of the elements of the input tensor with the given probability, helping prevent overfitting. A value of 0.0 indicates no dropout is applied.\n",
        "\n",
        "Setting a Manual Seed:\n",
        "\n",
        "`torch.manual_seed(1337)`: Sets the seed for generating random numbers in PyTorch. This ensures that the model's initialization and any other random operations are reproducible across runs for debugging and comparison purposes.\n",
        "\n",
        "These parameters together define the structure and training behavior of a Transformer-based model. Adjusting these hyperparameters can significantly affect model performance, training speed, and resource requirements."
      ],
      "metadata": {
        "id": "ZoQduAIPRHVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function estimate loss\n",
        "The code defines a function `estimate_loss` that estimates the average loss of a model on the training and validation datasets without updating the model's weights. It's designed to be run during or after training to monitor the model's performance.\n",
        "\n",
        "\n",
        "```\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's break down what it does:\n",
        "\n",
        "`@torch.no_grad()` Decorator:\n",
        "\n",
        "This decorator is applied to the estimate_loss function to disable gradient computation within the function. Disabling gradients saves memory and computations, making evaluation faster since backpropagation (needed for training) is not performed.\n",
        "\n",
        "Function Definition `def estimate_loss()`:\n",
        "\n",
        "Defines the function `estimate_loss` that, when called, estimates and returns the average loss for both the training and validation splits.\n",
        "\n",
        "Initialization of the Output Dictionary:\n",
        "\n",
        "`out = {}` initializes an empty dictionary to store the average loss for each data split ('train' and 'val').\n",
        "\n",
        "Model Evaluation Mode:\n",
        "\n",
        "`model.eval()` sets the model to evaluation mode, affecting layers like dropout and batch normalization, which behave differently during training vs. evaluation.\n",
        "\n",
        "Iterating Over Data Splits:\n",
        "\n",
        "The loop for split in `['train', 'val']`: iterates over two splits: training and validation.\n",
        "\n",
        "Loss Calculation for Each Split:\n",
        "\n",
        "Initializes a tensor losses = `torch.zeros(eval_iters)` to store the loss values for each iteration in evaluating the specified split.\n",
        "\n",
        "Iterates `eval_iters` times, each time retrieving a batch of data `X`, `Y` through `get_batch(split)`, which is assumed to be a function that provides batches of input data `X` and target labels `Y` for the specified split.\n",
        "\n",
        "`logits, loss = model(X, Y)` calculates the model's output logits and the corresponding loss `loss` for the given batch. The model is expected to return both the raw predictions (`logits`) and the calculated loss value when provided with inputs and targets.\n",
        "\n",
        "`losses[k] = loss.item()` stores the loss value for the current iteration by converting the loss tensor to a Python scalar using `.item()`.\n",
        "Storing the Mean Loss:\n",
        "\n",
        "After iterating through `eval_iters` batches, the mean loss for the split is calculated as `losses.mean()` and stored in the `out` dictionary with the split name as the key.\n",
        "\n",
        "Switching Back to Training Mode:\n",
        "\n",
        "`model.train()` sets the model back to training mode, re-enabling the training-specific behaviors like dropout and batch normalization updates, which were disabled during evaluation.\n",
        "\n",
        "Return Statement:\n",
        "\n",
        "Finally, the function returns the `out` dictionary containing the average loss for both the training and validation splits.\n",
        "\n",
        "This function is useful for monitoring model performance without affecting its training state, providing insights into how well the model is learning and generalizing to unseen data."
      ],
      "metadata": {
        "id": "Dnc7b2X7S2tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Head\n",
        "\n",
        "This code defines a class `Head`, which represents a single head of self-attention within a Transformer architecture. The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence differently for each element in the sequence. This class is implemented as a subclass of nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's break down its components:\n",
        "\n",
        "`Initialization Method __init__(self, head_size)`\n",
        "\n",
        "Parameters:\n",
        "- `head_size`: The size of the head, which determines the dimensionality of the key, query, and value vectors.\n",
        "\n",
        "Attributes:\n",
        "`self.key`, `self.query`, `self.value`: These are linear layers (without bias) that transform the input tensor `x` into key, query, and value representations, respectively. Each transformation projects the input embeddings (`n_embd`) into a lower-dimensional space (`head_size`).\n",
        "- `self.tril`: A lower triangular matrix registered as a buffer. This matrix is used for masking to ensure that each position in the sequence can only attend to preceding positions, enforcing causality in the attention mechanism. The use of `torch.tril(torch.ones(block_size, block_size))` creates a square matrix where each element below the main diagonal is 1 (inclusive), and all others are 0.\n",
        "- `self.dropout`: A dropout layer applied to the attention weights to prevent overfitting by randomly setting elements of the attention matrix to zero during training.\n",
        "\n",
        "Forward Method `forward(self, x)`:\n",
        "\n",
        "Input:\n",
        "`x`: The input tensor with shape (`B, T, C`), where `B` is the batch size, `T` is the sequence length, and `C` is the number of features (`embedding dimension`).\n",
        "\n",
        "Process:\n",
        "The input `x` is first transformed into key (`k`), query (`q`), and value (`v`) representations using the linear transformations defined in `__init__`.\n",
        "\n",
        "Attention scores are computed by taking the dot product of queries and keys `(q @ k.transpose(-2,-1))` and then scaling by the inverse square root of the dimensionality `(C**-0.5)`. This scaling factor is used to stabilize gradients.\n",
        "\n",
        "The resulting attention scores are masked with `-inf` where the lower triangular matrix `(self.tril)` is zero, ensuring causality. This masking before the softmax operation effectively removes these positions from consideration by making their weights zero after softmax is applied.\n",
        "\n",
        "A softmax function is applied to the masked attention scores along the last dimension to obtain the final attention weights, which are then passed through a dropout layer.\n",
        "\n",
        "The attention weights are used to perform a weighted aggregation of the value vectors, producing the output of the self-attention head.\n",
        "\n",
        "Output:\n",
        "\n",
        "The method returns the output tensor `out`, which is the result of applying self-attention to the input. This tensor has the same shape as the input (`B, T, C`) and contains the aggregated information based on the computed attention weights.\n",
        "\n",
        "This class encapsulates the functionality of a single attention head, focusing on computing weighted sums of value vectors based on the similarity between queries and keys, while respecting the sequential nature of the input through masking."
      ],
      "metadata": {
        "id": "WCL4MlgiUjGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHead Attention\n",
        "\n",
        "\n",
        "This code defines a class MultiHeadAttention, which represents a multi-head self-attention mechanism within a Transformer model. Multi-head attention allows the model to simultaneously attend to information from different representation subspaces at different positions. Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "```\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "```\n",
        "Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "Class Definition and Initialization Method `__init__(self, num_heads, head_size)`:\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- `num_heads`: The number of parallel attention heads.\n",
        "- `head_size`: The size of each attention head.\n",
        "\n",
        "Attributes:\n",
        "- `self.heads`: A nn.ModuleList containing instances of the Head class (defined previously). Each Head instance represents a single self-attention head. The list size is determined by `num_heads`, allowing for parallel computation of different attention \"views.\"\n",
        "`self.proj`: A linear layer that projects the concatenated output of all attention heads back to the original embedding dimension (n_embd). This projection is necessary because the concatenated outputs from all heads increase the dimensionality, and we often want the output of the multi-head attention to have the same dimensionality as the input for residual connections and further processing.\n",
        "- `self.dropout`: A dropout layer applied after the projection to prevent overfitting by randomly zeroing out elements of the output tensor during training.\n",
        "\n",
        "Forward Method `forward(self, x)`:\n",
        "\n",
        "Input:\n",
        "\n",
        "`x`: The input tensor with shape (`B, T, C`), where `B` is the batch size, `T` is the sequence length, and `C` is the number of features (embedding dimension).\n",
        "\n",
        "Process:\n",
        "\n",
        "The input `x` is processed by each attention head in `self.heads`, resulting in `num_heads` output tensors.\n",
        "These outputs are concatenated along the last dimension `(dim=-1)`. Since each head potentially transforms the input into a different subspace (`head_size`), concatenating these allows the model to combine diverse information from different subspaces.\n",
        "\n",
        "The concatenated output is then projected back to the original embedding dimension (`n_embd`) using `self.proj`.\n",
        "Dropout is applied to the projected output as a regularization measure.\n",
        "\n",
        "Output:\n",
        "\n",
        "The method returns the final output tensor, which has undergone multi-head self-attention and been projected back to the original embedding dimensionality. This output can be used for further processing or as part of a larger model, like a Transformer block.\n",
        "\n",
        "This class effectively combines information from multiple perspectives (`heads`) on the input sequence, enhancing the model's ability to capture various dependencies and features within the data. Multi-head attention is a key component of Transformer architectures, contributing to their effectiveness in handling complex sequence-based tasks."
      ],
      "metadata": {
        "id": "gtP9rscCWuUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block(nn.module)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "```\n"
      ],
      "metadata": {
        "id": "KzffiDgiY8S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigramLanguageModel\n",
        "This code defines a `BigramLanguageModel` class, which is a simplified transformer-based model intended for language modeling tasks. The model aims to predict the next token in a sequence based on the previous tokens, implementing a structure reminiscent of Transformer models but tailored for a specific context of generating or evaluating sequences token-by-token.\n",
        "\n",
        "```\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "```\n",
        "Let's dissect its components and functionalities:\n",
        "\n",
        "- Initialization Method `__init__(self)`:\n",
        "\n",
        "Attributes:\n",
        "\n",
        "- `self.token_embedding_table`: An embedding layer for tokens. It maps tokens from a vocabulary of size vocab_size to embeddings of size n_embd.\n",
        "- `self.position_embedding_table`: An embedding layer for positions within a sequence. It maps position indices (up to block_size) to embeddings of the same size n_embd, enabling the model to understand the order of tokens.\n",
        "- `self.blocks`: A sequential container of Block instances. Each Block is presumably a Transformer block (not defined in this snippet), which includes self-attention and feedforward layers, applied successively to the input embeddings. The number of blocks is n_layer, allowing for multiple layers of processing.\n",
        "- `self.ln_f`: A layer normalization applied to the final output of the Transformer blocks. It helps stabilize the hidden state distributions before the final prediction.\n",
        "- `self.lm_head`: A linear projection layer that maps the output of the Transformer blocks back to the vocabulary space. This is used to produce logits for each token in the vocabulary.\n",
        "\n",
        "Forward Method `forward(self, idx, targets=None)`:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "-`idx`: A tensor of shape (B, T) containing indices of input tokens.\n",
        "- `targets`: An optional tensor of shape (B, T) containing indices of target tokens for training. If `targets` is not provided, the method assumes inference mode and does not compute loss.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Embeds both tokens and their positions, then sums these embeddings to produce a representation that contains information about both the identity and the order of tokens.\n",
        "- Processes the embedded input through the Transformer blocks `(self.blocks)`.\n",
        "- Applies layer normalization `(self.ln_f)`.\n",
        "- Projects the normalized output to the vocabulary space `(self.lm_head)`, producing logits for each token position.\n",
        "- If `targets` are provided, computes the cross-entropy loss between the predicted logits and the true targets. This is done after reshaping logits and targets to ensure compatibility with the loss function.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "- `logits`: The logits corresponding to the probability distribution over the vocabulary for each token position.\n",
        "- `loss`: The computed cross-entropy loss if targets are provided, otherwise None.\n",
        "\n",
        "Generate Method `generate(self, idx, max_new_tokens)`:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "`idx`: A tensor of shape (B, T) containing the starting sequence of token indices.\n",
        "- `max_new_tokens`: The maximum number of new tokens to generate.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Iteratively generates `max_new_tokens` by predicting one token at a time and appending it to the sequence.\n",
        "- At each step, limits the context to the last `block_size` tokens to manage computation and memory efficiency, and to adhere to the model's design constraints.\n",
        "- Uses the model's forward pass to get logits for the next token, applies `softmax` to convert logits into probabilities, and then samples a new token index from this probability distribution.\n",
        "- Concatenates the newly sampled token to the existing sequence and repeats until max_new_tokens are generated.\n",
        "\n",
        "Output:\n",
        "\n",
        "- Returns the extended sequence with the newly generated tokens appended.\n",
        "\n",
        "This class encapsulates the functionalities required for both training a language model (via the forward method) and generating text (via the generate method), showcasing a basic yet powerful application of Transformer architecture principles in NLP tasks.\n"
      ],
      "metadata": {
        "id": "cxSntzrnFget"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training Loop\n",
        "\n",
        "It code below outlines the procedure for training the model on a dataset, evaluating its performance periodically, and finally generating text based on a given context.\n",
        "\n",
        "```\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "```\n",
        "\n",
        "Let's break down the code into its key components:\n",
        "\n",
        "**Training Loop**\n",
        "\n",
        "Iteration Loop:\n",
        "\n",
        "- `for iter in range(max_iters)`: iterates through a specified number of training iterations or epochs (`max_iters`).\n",
        "\n",
        "Conditional Evaluation:\n",
        "\n",
        "- `if iter % eval_interval == 0 or iter == max_iters - 1`: checks if the current iteration is a multiple of the evaluation interval (eval_interval) or the last iteration. If so, the model's performance is evaluated.\n",
        "\n",
        "- `losses = estimate_loss()` calls a function to estimate the model's loss on both the training and validation datasets. This function is likely similar to the estimate_loss function explained earlier.\n",
        "The training and validation losses are printed to monitor the model's performance over time.\n",
        "Batch Processing:\n",
        "\n",
        "`xb, yb = get_batch('train')` fetches a batch of input data (`xb`) and corresponding targets (`yb`) for training.\n",
        "\n",
        "Loss Calculation and Optimization:\n",
        "\n",
        "- `logits, loss = model(xb, yb)` computes the model's predictions (`logits`) and the loss (`loss`) for the current batch.\n",
        "\n",
        "- `optimizer.zero_grad(set_to_none=True)` clears any old gradients from the previous step to prevent accumulation.\n",
        "\n",
        "- `loss.backward()` computes the gradient of the loss with respect to the model parameters.\n",
        "\n",
        "- `optimizer.step()` updates the model parameters based on the gradients.\n",
        "\n",
        "**Text Generation**\n",
        "\n",
        "Initialization:\n",
        "\n",
        "`context = torch.zeros((1, 1), dtype=torch.long, device=device)` initializes a tensor to serve as the starting context for text generation. It's set to a single token of zeros, indicating an empty context or start token.\n",
        "\n",
        "Generation Loop:\n",
        "\n",
        "- `m.generate(context, max_new_tokens=2000)` generates new tokens starting from the provided context for a maximum of 2000 new tokens. The generate method likely uses the model to predict the next token based on the current sequence, samples a token from the predicted probabilities, and appends it to the sequence. This process is repeated until the maximum number of new tokens is reached.\n",
        "\n",
        "Decoding and Printing:\n",
        "\n",
        "- `decode(m.generate(context, max_new_tokens=2000)[0].tolist())` decodes the generated indices back into readable text. The decode function is assumed to map numerical token IDs back to their corresponding string representations. The generated text is then printed.\n",
        "\n",
        "This training and generation loop is typical for neural network-based language models, combining periods of training with evaluation to monitor progress and adjusting model parameters to minimize loss. The final generation step showcases the model's ability to produce coherent and contextually relevant text sequences."
      ],
      "metadata": {
        "id": "iebQCgmqODlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "- [Attention is All You Need paper](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "- [OpenAI GPT-3 paper](https://arxiv.org/abs/2005.14165 )\n",
        "\n",
        "- [OpenAI ChatGPT blog post](https://openai.com/blog/chatgpt/)\n",
        "\n",
        "- [The illustrated transfomer](https://jalammar.github.io/illustrated-transformer/)"
      ],
      "metadata": {
        "id": "TBlQ59oEQJ0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "oG7PTMVtTd8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghwvoe5xZJAE"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "# Not really an LM at this stage, but we will get there...\n",
        "class BigramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Embedding the vocabulary\n",
        "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
        "\n",
        "    def forward(self, idx, targets):\n",
        "        # idx is of shape (batch_size, block_size)\n",
        "        # targets is of shape (batch_size, block_size)\n",
        "        # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
        "        logits = self.embed(idx)\n",
        "        return logits\n",
        "\n",
        "\n",
        "print('Vocabulary size:', vocab_size)  # Length of the vocabulary list (this includes the space character)\n",
        "m = BigramLM(vocab_size)  # Instantiate the model\n",
        "out = m(xb, yb)           # Forward pass (yb remains unused for now)\n",
        "print(out.shape)          # (batch_size, block_size, vocab_size) -> 4 times 8 characters, each embedded as a 65-dim vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db0a-h-2ZJAE"
      },
      "source": [
        "Every integer of our tokenized text is now represented by an embedding vector of size `vocab_size`.<br>\n",
        "We do this by using an embedding layer. This layer is effectively a lookup table that maps<br>\n",
        "each possible (`vocab_size` are possible in total) character-representing index to a unique vector of size `vocab_size`.\n",
        "\n",
        "The `logits` are the outputs of the model.<br>\n",
        "We just treat the embedded tokens of the input batch as the logits.<br>\n",
        "This `logits` tensor holds all the embedded identities of the tokens in the input batch -> ($batch\\_size \\times block\\_size \\times vocab\\_size$).\n",
        "\n",
        "We are *not yet* interconnecting the tokens with any sort of model/logic.<br>\n",
        "We are not yet training or predicting *anything*.\n",
        "\n",
        "This is about to change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgLUFJ2ZJAE"
      },
      "source": [
        "## Setting up a Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_R6KACmZJAE"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, vocab_size)  # Embedding the vocabulary, each individual token is represented by a vector of size vocab_size\n",
        "\n",
        "    def forward(self, idx, targets):\n",
        "        logits = self.embed(idx)      # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
        "        B, T, C = logits.shape        # B = batch_size, T = block_size, C = vocab_size\n",
        "        logits = logits.view(B*T, C)  # Transpose logits to (B*T, C)\n",
        "        # This is the first time we actively use the targets:\n",
        "        targets = targets.view(B*T)   # Transpose targets to (B*T) (targets contains the next token's index for each input sequence in the batch)\n",
        "        loss = F.cross_entropy(logits, targets)  # Calculating cross entropy loss across all tokens in the batch (using targets to plug out the correct token for each input sequence)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "m = BigramLM(vocab_size)  # Instantiate the model\n",
        "logits, loss = m(xb, yb)  # Forward pass (xb becomes embedded, yb is used to calculate the loss)\n",
        "print(logits.shape)       # (batch_size * block_size, vocab_size)\n",
        "print(loss.item())        # Loss value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDomPfFWZJAF"
      },
      "source": [
        "Given that we have the identities of the next character through `yb`, how well does the model predict them through the `logits`? The `loss` is the measurement of prediction quality.\n",
        "\n",
        "We want the index within `yb` to be the same as the most likely/active index within `logits`.<br>\n",
        "The loss is measured as the average of this across all the tokens in the input batch.\n",
        "\n",
        "We know the `vocab_size` is $65$.<br>\n",
        "We can calculate what the loss should be if we were to predict the next token totally randomly:\n",
        "\n",
        "$$-ln(\\frac{1}{65}) = 4.1743872699$$\n",
        "\n",
        "Our calculated loss is **higher/worse**, because we are not predicting perfectly randomly to begin with.<br>\n",
        "The initial predictions are not perfectly spread out across the `vocab_size`.<br>\n",
        "They aren't super diffuse and contain a bit of entropy.<br>\n",
        "We haven't yet learned uniform distribution across the `vocab_size`.\n",
        "\n",
        "![](https://images.squarespace-cdn.com/content/56316c94e4b098620a45e78a/1457973972468-D5XJVA1ABFXSD0AH9RZC/?content-type=image%2Fpng)\n",
        "<br>Source: [Shiken](https://shiken.ai/chemistry/entropy)\n",
        "\n",
        "The `loss` is to be minimized.<br>\n",
        "We will need the model to make predictions of individual next tokens.<br>\n",
        "\n",
        "Let's append the current model with a function `generate` that takes in the last token of a sequence and returns the next token however many times we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "843S5bYJZJAF"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, vocab_size)      # Embedding the vocabulary, each individual token is represented by a vector of size vocab_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.embed(idx)                               # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)                       # Transpose logits to (B, C, T) (B=batch_size, T=block_size, C=vocab_size)\n",
        "            targets = targets.view(B*T)                        # Transpose targets to (B, T)\n",
        "            loss = F.cross_entropy(logits, targets)            # Calculating cross entropy loss across all tokens in the batch\n",
        "        return logits, loss\n",
        "\n",
        "    # Generate new tokens based on respective last token of a sequence\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)                              # Forward pass (this is the forward function) with the current sequence of characters idx, results in (B, T, C)\n",
        "            logits = logits[:, -1, :]                          # Focus on the last token from the logits (B, T, C) -> (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)                  # Calculate the probability distribution for the next token based on this last token, results in (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Sample the next token (B, 1), the token with the highest probability is sampled most likely\n",
        "            idx = torch.cat((idx, idx_next), dim=1)            # Add the new token to the sequence (B, T+1) for the next iteration\n",
        "        return idx                                             # Return the sequence of tokens (B, T+1), these are characters\n",
        "\n",
        "m = BigramLM(vocab_size)  # Instantiate the model\n",
        "logits, loss = m(xb, yb)  # Forward pass\n",
        "\n",
        "print(logits.shape)       # (batch_size, block_size, vocab_size)\n",
        "print(loss) # Loss value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlyw2oCHZJAF"
      },
      "source": [
        "Let's recap this `generate` function:<br>\n",
        "The function takes in a batch of tokens `xb` and a number of tokens to generate `n`.\n",
        "\n",
        "Repeated over `n` times, it will:\n",
        "- forward pass through the model with tokens `xb` to get `logits`\n",
        "- disregard everything but the last token of `xb`\n",
        "- calculate the probability of each possible token in the vocabulary to be the token after this last `xb` token; this is done with `F.softmax`\n",
        "- sample a token from the probability distribution with `torch.multinomial`, this returns an index of the token that we can use to look up the token itself in the vocabulary if we wanted\n",
        "- append the sampled token to the tokens `xb`\n",
        "- repeat\n",
        "\n",
        "See that `self(idx)` calls the `forward` function of the model. `forward` is adapted accordingly above to also take a call with just `idx`.\n",
        "\n",
        "Let's run this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL43lLdqZJAF"
      },
      "source": [
        "## Producing The First Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdXjx-3xZJAF"
      },
      "outputs": [],
      "source": [
        "ix = torch.zeros((1, 1), dtype=torch.long)  # Start with a single tensor of shape (1, 1) holding a 0 (new line)\n",
        "tokens = m.generate(ix, max_new_tokens=100) # Generate 100 tokens as a sequence of indices\n",
        "print(tokens.shape)                         # Print the shape of the resulting sequence of tokens\n",
        "print(decode(tokens[0].tolist()))           # Decode the resulting sequence of indices to a string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzM5PPiBZJAF"
      },
      "source": [
        "We do the most basic generative task here:<br>\n",
        "We feed the model a prompt of just the newline character and let it iteratively<br>\n",
        "generate 100 'most probable' characters as a follow-up.<br>\n",
        "\n",
        "Within the print-statement there is the `[0]` call. This is **not** because we are only interested in a first character of the generated text or anything like that.<br>\n",
        "It is because `generate` returns a tensor of size `batch_size x 101`. We only have a `batch_size` of $1$ here, so we can just take the first element of the array and convert it to a string.\n",
        "\n",
        "\n",
        "The generation as is right now is not very good.<br>\n",
        "The `generate` function loops, increases the `context_size` and always re-feeds itself with this growing context.<br>\n",
        "Yet, with the logits generated from that we are not taking anything beyond/prior the logits of the last token from the context as basis for our prediction.\n",
        "\n",
        "For the current approach, our context could be of fixed size. With the current (bigram) model, we are not using the context to its full potential. This will be addressed soon.\n",
        "\n",
        "For now, let's train!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "DPGmqquBohI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG4FaGUBZJAG"
      },
      "outputs": [],
      "source": [
        "# Create a PyTorch Optimizer\n",
        "# Instantiate AdamW optimizer with the model parameters (weights)\n",
        "# and a learning rate of 0.001 (often used value for *small* networks)\n",
        "opt = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ_K1JyUZJAG"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # Increasing the batch size from 4 to 32\n",
        "losses = []\n",
        "\n",
        "# Train for 10000 steps/batches\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train', batch_size) # Sample a batch of data\n",
        "    logits, loss = m(xb, yb)                # Forward pass, calculate the loss\n",
        "    loss.backward()                         # Backprop with PyTorch's autograd\n",
        "                                            # (effectively just updating the logits/the embedding vectors)\n",
        "    opt.step()                              # Update the weights\n",
        "    opt.zero_grad()                         # Set the gradients to zero\n",
        "\n",
        "    # Print the loss every 100 steps\n",
        "    if steps % 100 == 0:\n",
        "        print(f'Loss at step {steps}: {loss.item()}')\n",
        "        losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuXnDMVvZJAG"
      },
      "source": [
        "Let's now sample from the model and see how it performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va0dwFCLZJAP"
      },
      "outputs": [],
      "source": [
        "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSgfHdl3ZJAP"
      },
      "source": [
        "We currently exclusively embed the tokens into randomly generated 65-dimensional vectors.<br>\n",
        "When we then enter a batch of $4$ token sequences, where each sequence is $8$ character indices long.<br>\n",
        "We embed the indices to receive a tensor of size $[4 \\times 8 \\times 65]$.<br>\n",
        "We then reshape this tensor to $[32 \\times 65]$ and compare that with the target tensor of (also reshaped) size $32$.<br>\n",
        "The loss is then the determined by the `CrossEntropyLoss` function, which effectively plugs out the probability of the target token from the `logits` with index of the target token and then takes the negative logarithm of that.<br>\n",
        "The loss is then averaged across all the tokens in the batch. A scalar value is returned.\n",
        "\n",
        "We build a model that optimizes the embedding vectors to carry the highest probabilities for the most likely next token(s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao_aE0oxZJAQ"
      },
      "source": [
        "Remember, we only increased `batch_size` and trained for more epochs.<br>\n",
        "The model is still the same. We still only predict the next token based on the previous token.\n",
        "\n",
        "**There is one thing to say about the loss:**<br>\n",
        "At this point, the loss is very noisy. This is due to every batch being (independently) more or less lucky with predictions.<br>\n",
        "Viewed across the entire training, the loss is not really comparable across batches, making the loss jumpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyeuV-L_ZJAQ"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4SuvNSCZJAQ"
      },
      "source": [
        "Consider that this is sampled loss after every $100$ steps.<br>\n",
        "The loss we visualize here is too batch-specific and each batch is too small to be representative of the entire training.<br>\n",
        "We see a trend though. At this point, switch over to `bigram.py` to see our code in execution-optimized script form.<br>\n",
        "There, this loss interpretation problem is addressed like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIaO_lbsZJAQ"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "\n",
        "@torch.no_grad() # Disable gradient calculation for this function\n",
        "def evaluate_loss():\n",
        "    out = {}\n",
        "    m.eval() # Set model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size)\n",
        "            _, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train() # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "# Training\n",
        "for iter in range(max_iters):\n",
        "    xb, yb = get_batch('train', batch_size) # Get batch\n",
        "    logits, loss = m(xb, yb)                # Forward pass\n",
        "    loss.backward()                         # Backward pass\n",
        "    opt.step()                        # Update parameters\n",
        "    opt.zero_grad(set_to_none=True)   # Reset gradients\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = evaluate_loss()\n",
        "        train_losses.append(losses[\"train\"].item())\n",
        "        print(f'Iter {iter:4d} | Train Loss {losses[\"train\"]:6.4f} | Val Loss {losses[\"val\"]:6.4f}')\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long) # Start with a zero context\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-FcaRZMZJAQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DGBpaD1ZJAQ"
      },
      "source": [
        "Instead of just printing the loss batch-wise, `evaluate_loss` averages the loss across `eval_iter` batches.<br>\n",
        "For `eval_iter` times, `evaluate_loss` will sample a batch of tokens, run the current model on it and average the loss.\n",
        "\n",
        "This is done for both the training and validation set.\n",
        "\n",
        "It is more accurate to do this, because now we start to see the trend of the loss in context of multiple, randomly sampled batches and thus a broader representation of the dataset.<br>\n",
        "As the loss is averaged over multiple batches, it is also less noisy.\n",
        "\n",
        "With all of this, our `bigram.py` script is a great starter for bulding a GPT."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# from torch.nn import functional as F\n",
        "# torch.manual_seed(1337)\n",
        "\n",
        "# class BigramLanguageModel(nn.Module):\n",
        "#     # initializing an embedding table that will create embeddings for each token in the vocabulary\n",
        "#     # The embeddings have the same dimension as the vocabulary size, which means\n",
        "#     # this model tries to predict the next token using a simplified one-hot-like encoding approach.\n",
        "#     def __init__(self, vocab_size):\n",
        "#         super().__init__()\n",
        "#         # each token directly reads off the logits for the next token from a lookup table\n",
        "#         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "#     # The forward method defines the computation performed at every call.\n",
        "#     # It takes indices of tokens (idx) and uses the embedding table to look up the logits for the next tokens.\n",
        "#     def forward(self, idx, targets=None):\n",
        "#         # idx and targets are both (B,T) tensor of integers\n",
        "#         # predicting the next token\n",
        "#         logits = self.token_embedding_table(idx)  # (B,T,C) (batch x time x tensor) that is 4 x 8 x 65 channel C is the vocab_size\n",
        "#         B, T, C = logits.shape\n",
        "#         if targets is None:\n",
        "#           loss = None\n",
        "#         else:\n",
        "#           logits = logits.view(B*T, C)\n",
        "#           targets = targets.view(B*T)\n",
        "#           loss = F.cross_entropy(logits, targets)\n",
        "#         return logits, loss #return the scores for the next character in the sequence\n",
        "\n",
        "#     def generate(self, idx, max_new_tokens):\n",
        "#     # idx is (B, T) array of indices in the current context\n",
        "#         for _ in range(max_new_tokens):\n",
        "#           logits, loss = self(idx)\n",
        "#           # focus only on the last time step\n",
        "#           logits = logits[:, -1, :]  # becomes (B, C)\n",
        "#           # apply softmax to get probabilities\n",
        "#           probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "#           # sample from the distribution\n",
        "#           idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "#           # append sampled index to the running sequence\n",
        "#           idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "#           return idx\n",
        "\n",
        "\n",
        "# # an instance of the BigramLanguageModel is created\n",
        "# m = BigramLanguageModel(vocab_size)\n",
        "# logits, loss = m(xb, yb) # passing the inputs and t\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "\n",
        "# print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "YCbDemP0MtmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB:\n",
        "`generate` is a method and does the following:\n",
        "- It takes an input tensor `idx` of shape (B, T), where B is the batch size and T is the sequence length, representing indices of tokens in the current context.\n",
        "- `max_new_tokens` specifies how many new tokens should be generated.\n",
        "Inside the loop, the model generates logits and (presumably ignored) loss for the current indices.\n",
        "- It slices the logits to focus on the last set of predictions (the last time step) for each sequence in the batch.\n",
        "- A softmax is applied to the sliced logits to convert them into probabilities.\n",
        "- torch.multinomial is used to sample from the probability distribution given by probs, effectively picking the next token index for each sequence.\n",
        "- The sampled index idx_next is concatenated with the current indices idx, extending each sequence in the batch by one token.\n",
        "\n",
        "After generating the specified number of new tokens, the updated idx tensor, which contains the original context plus the new tokens, is returned.\n"
      ],
      "metadata": {
        "id": "cFm-zvqdPmQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "VkEn_QvjY7H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "id": "tukiH-NbRBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "Hs_E24uRE8kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "id": "yhdOAd6-wXkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "id": "wOURrfG-ysoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "EDarxEWIRMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "id": "vT1hdtzXCjgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "id": "Nl6I9n9IRTSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "id": "T1tQx7oeRvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "id": "MLb_odHU3iKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "id": "JB82yzt44REI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "id": "Mpt8569BB9_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "2Num7sX9CKOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "id": "633T2cmnW1uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "id": "LN9cK9BoXCYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "hoelkOrFY8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}